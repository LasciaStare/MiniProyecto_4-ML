{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "035d884d",
   "metadata": {},
   "source": [
    "# Modelado con PySpark (Versión Optimizada)\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Este notebook implementa el pipeline de machine learning para la clasificación de préstamos utilizando PySpark. Esta versión se centra en optimizar el rendimiento y la eficiencia en un entorno local.\n",
    "\n",
    "1.  **Iniciar una SparkSession** optimizada para el uso de todos los recursos locales.\n",
    "2.  **Cargar** el conjunto de datos y aplicar técnicas de **cache y repartitioning**.\n",
    "3.  **Construir un `Pipeline` de PySpark** para la transformación de características.\n",
    "4.  **Entrenar un `RandomForestClassifier`** y optimizar sus hiperparámetros usando `TrainValidationSplit` para mayor velocidad.\n",
    "5.  **Evaluar** el rendimiento del modelo en el conjunto de prueba.\n",
    "6.  **Guardar** el modelo entrenado y sus métricas.\n",
    "\n",
    "-   **Entrada:** `data/processed/loans_cleaned.parquet`\n",
    "-   **Salidas:**\n",
    "    -   `models/pyspark_random_forest/` (Carpeta con el modelo de Spark)\n",
    "    -   `results/pyspark_metrics.json` (Métricas de evaluación y tiempo de entrenamiento)\n",
    "\n",
    "### 1. Importación de Librerías e Inicio de SparkSession\n",
    "\n",
    "Iniciamos una `SparkSession` con configuraciones específicas para mejorar el rendimiento en una máquina local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9df7447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HADOOP_HOME'] = 'C:\\\\hadoop'\n",
    "os.environ['PATH'] += ';C:\\\\hadoop\\\\bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddbfc8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark iniciado. Usando 12 cores.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession, types as T\n",
    "from pyspark.ml import Pipeline as SparkPipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Imputer\n",
    "from pyspark.ml.classification import RandomForestClassifier as SparkRandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "\n",
    "# Ajustes para optimización local:\n",
    "# - master(\"local[*]\"): Usa todos los cores disponibles en la máquina.\n",
    "# - config(\"spark.driver.memory\", \"10g\"): Aumenta la memoria asignada al driver.\n",
    "# - setLogLevel(\"WARN\"): Reduce la cantidad de logs para una salida más limpia.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySparkLoanClassification\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# Ajustar particiones de shuffle: un parámetro clave para el rendimiento.\n",
    "# Evita tener demasiadas particiones pequeñas, lo que genera overhead.\n",
    "num_cores = max(1, spark.sparkContext.defaultParallelism)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", str(max(8, num_cores * 4)))\n",
    "\n",
    "print(f\"Spark iniciado. Usando {num_cores} cores.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dab6db3",
   "metadata": {},
   "source": [
    "### 2. Definición de Rutas y Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5293ed65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros totales: 1345310\n"
     ]
    }
   ],
   "source": [
    "PROCESSED_DATA_PATH = 'data/processed/loans_cleaned.parquet'\n",
    "MODEL_PATH = 'models/pyspark_random_forest'\n",
    "RESULTS_PATH = 'results/pyspark_metrics.json'\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "\n",
    "df_spark = spark.read.parquet(PROCESSED_DATA_PATH)\n",
    "# Evitamos llamar a .count() repetidamente, ya que es una acción costosa.\n",
    "print(f\"Registros totales: {df_spark.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faec3593",
   "metadata": {},
   "source": [
    "### 3. Preparación de Datos: División, Repartición y Cache\n",
    "\n",
    "Este es un paso crucial para el rendimiento. Dividimos los datos y luego los `repartition` y `cache`.\n",
    "-   **`repartition`**: Asegura que los datos se distribuyan uniformemente entre los cores, maximizando el paralelismo.\n",
    "-   **`cache`**: Almacena el DataFrame en memoria después de la primera acción. Esto evita que Spark tenga que volver a calcularlo desde el principio en cada paso posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "340001cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros de entrenamiento (cacheados): 1076235\n",
      "Registros de prueba (cacheados): 269075\n"
     ]
    }
   ],
   "source": [
    "# Dividir datos una sola vez y cachear; repartir para buen paralelismo\n",
    "train_data, test_data = df_spark.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Reparticionamos para que cada core tenga una cantidad de trabajo razonable\n",
    "train_data = train_data.repartition(num_cores * 2).cache()\n",
    "test_data = test_data.repartition(num_cores * 2).cache()\n",
    "\n",
    "# Spark es \"lazy\", por lo que las transformaciones no se ejecutan hasta que se llama a una acción.\n",
    "# Usamos .count() para \"materializar\" la caché y forzar la ejecución de los pasos anteriores.\n",
    "print(f\"Registros de entrenamiento (cacheados): {train_data.count()}\")\n",
    "print(f\"Registros de prueba (cacheados): {test_data.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3632c92",
   "metadata": {},
   "source": [
    "### 4. Construcción del Pipeline de PySpark ML\n",
    "\n",
    "El pipeline define las etapas de preprocesamiento y modelado. Usamos una forma más robusta de identificar los tipos de columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5bb25a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas Categóricas: 3, Columnas Numéricas: 6\n"
     ]
    }
   ],
   "source": [
    "label_col = 'default'\n",
    "features = [c for c in df_spark.columns if c != label_col]\n",
    "\n",
    "categorical_cols = []\n",
    "numerical_cols = []\n",
    "\n",
    "# Identificar columnas por su tipo de dato real en el schema\n",
    "for field in df_spark.schema.fields:\n",
    "    if field.name in features:\n",
    "        if isinstance(field.dataType, T.StringType):\n",
    "            categorical_cols.append(field.name)\n",
    "        elif isinstance(field.dataType, (T.IntegerType, T.DoubleType, T.FloatType, T.LongType, T.ShortType)):\n",
    "            numerical_cols.append(field.name)\n",
    "\n",
    "print(f\"Columnas Categóricas: {len(categorical_cols)}, Columnas Numéricas: {len(numerical_cols)}\")\n",
    "\n",
    "# --- Etapas del Pipeline ---\n",
    "imputer = Imputer(strategy='median',\n",
    "                  inputCols=numerical_cols,\n",
    "                  outputCols=[f\"{c}_imputed\" for c in numerical_cols])\n",
    "\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_index\", handleInvalid='keep') for c in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCol=f\"{c}_index\", outputCol=f\"{c}_vec\") for c in categorical_cols]\n",
    "\n",
    "assembler_inputs = [f\"{c}_imputed\" for c in numerical_cols] + [f\"{c}_vec\" for c in categorical_cols]\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\n",
    "\n",
    "rf = SparkRandomForestClassifier(labelCol=label_col, featuresCol=\"features\", seed=42)\n",
    "\n",
    "# Unir todas las etapas en un único pipeline\n",
    "all_stages = [imputer] + indexers + encoders + [assembler, rf]\n",
    "pipeline = SparkPipeline(stages=all_stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51545797",
   "metadata": {},
   "source": [
    "### 5. Entrenamiento y Ajuste de Hiperparámetros con TrainValidationSplit\n",
    "\n",
    "Para acelerar la búsqueda de hiperparámetros, usamos `TrainValidationSplit` en lugar de `CrossValidator`.\n",
    "-   `CrossValidator` entrena `k` modelos por cada combinación de hiperparámetros (ej: 3 pliegues = 3 entrenamientos).\n",
    "-   `TrainValidationSplit` divide los datos de entrenamiento una sola vez (ej: 80/20) y entrena solo un modelo por combinación. Es mucho más rápido y es ideal para desarrollos iniciales o datasets muy grandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbfc7445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando TrainValidationSplit...\n",
      "Entrenamiento completado en 484.3 segundos.\n"
     ]
    }
   ],
   "source": [
    "# Definir el grid de parámetros\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [50, 100]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=label_col, metricName=\"areaUnderROC\")\n",
    "\n",
    "# Configurar TrainValidationSplit\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    trainRatio=0.8,   # 80% de los datos de entrenamiento se usan para entrenar, 20% para validar\n",
    "    parallelism=max(1, num_cores // 2)  # Controla cuántos modelos se entrenan en paralelo\n",
    ")\n",
    "\n",
    "print(\"Iniciando TrainValidationSplit...\")\n",
    "start_time = time.time()\n",
    "model = tvs.fit(train_data)\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Entrenamiento completado en {training_time:.1f} segundos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0c96a8",
   "metadata": {},
   "source": [
    "### 6. Evaluación y Guardado del Modelo\n",
    "\n",
    "Finalmente, evaluamos el mejor modelo en el conjunto de prueba y guardamos tanto el modelo como las métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77021040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas de evaluación en el conjunto de prueba:\n",
      "- Accuracy: 0.8011\n",
      "- Precision: 0.6418\n",
      "- Recall: 0.8011\n",
      "- F1-score: 0.7126\n",
      "- ROC AUC: 0.6947\n",
      "\n",
      "Modelo guardado en: 'models/pyspark_random_forest'\n",
      "Resultados guardados en: 'results/pyspark_metrics.json'\n"
     ]
    }
   ],
   "source": [
    "# Realizar predicciones en el conjunto de prueba\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Calcular ROC AUC\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "# Función auxiliar para calcular otras métricas\n",
    "def get_multiclass_metric(metric_name, preds):\n",
    "    ev = MulticlassClassificationEvaluator(labelCol=label_col, predictionCol=\"prediction\", metricName=metric_name)\n",
    "    return ev.evaluate(preds)\n",
    "\n",
    "metrics = {\n",
    "    \"Accuracy\": get_multiclass_metric(\"accuracy\", predictions),\n",
    "    \"Precision\": get_multiclass_metric(\"weightedPrecision\", predictions),\n",
    "    \"Recall\": get_multiclass_metric(\"weightedRecall\", predictions),\n",
    "    \"F1-score\": get_multiclass_metric(\"f1\", predictions),\n",
    "    \"ROC AUC\": auc\n",
    "}\n",
    "\n",
    "print(\"Métricas de evaluación en el conjunto de prueba:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"- {metric}: {value:.4f}\")\n",
    "\n",
    "# Guardar el mejor modelo (pipeline completo)\n",
    "best_model_spark = model.bestModel\n",
    "best_model_spark.write().overwrite().save(MODEL_PATH)\n",
    "\n",
    "# Guardar los resultados en formato JSON\n",
    "results = {\"metrics\": metrics, \"training_time_seconds\": training_time}\n",
    "with open(RESULTS_PATH, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\nModelo guardado en: '{MODEL_PATH}'\")\n",
    "print(f\"Resultados guardados en: '{RESULTS_PATH}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8a1a77",
   "metadata": {},
   "source": [
    "### 7. Detener la Sesión de Spark\n",
    "\n",
    "Es una buena práctica detener explícitamente la `SparkSession` para liberar todos los recursos del clúster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8a8f9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession detenida.\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print(\"SparkSession detenida.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dced8a56",
   "metadata": {},
   "source": [
    "---\n",
    "### Fin del Notebook 3\n",
    "\n",
    "Hemos implementado un pipeline de modelado optimizado con PySpark, listo para ser comparado con el enfoque de Scikit-learn.\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "mlvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
